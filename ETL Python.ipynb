{"cells":[{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"3AnzTDM3HNuT"},"id":"3AnzTDM3HNuT"},{"cell_type":"code","source":["#!python config\n","\n","# TAHAP 1 : LENGKAPI İSİ TABELNYA ! DAN SUDAH LENGKAP :)\n","\n","oltp_conn_string = 'postgresql://spiderweb:spiderweb@47.237.106.32:5432/spiderweb_oltp'\n","warehouse_conn_string = 'postgresql://spiderweb:spiderweb@47.237.106.32:5432/spiderweb_dwh'\n","\n","oltp_tables = {\n","    \"users\": \"tb_users\",\n","    \"payments\": \"tb_payments\",\n","    \"shippers\": \"tb_shippers\",\n","    \"ratings\": \"tb_ratings\",\n","    \"vouchers\": \"tb_voucher\", # klo di DWH itu dim_voucher, bukan dim_vouchers\n","    \"orders\": \"tb_orders\",\n","    \"product_category\" : \"tb_product_category\",\n","    \"products\" : \"tb_products\",\n","    \"order_items\" : \"tb_order_items\"\n","}\n","\n","warehouse_tables = {\n","    \"users\": \"dim_users\", # diubah dari dim_user ke dim_users ( sesuai ERD kelompok 5)\n","    \"payments\": \"dim_payments\", # diubah dari dim_payment ke dim_payments ( sesuai ERD kelompok 5)\n","    \"shippers\": \"dim_shippers\", # diubah dari dim_shipper ke dim_shippers ( sesuai ERD kelompok 5)\n","    \"ratings\": \"dim_ratings\", # diubah dari dim_rating ke dim_ratings ( sesuai ERD kelompok 5)\n","    \"vouchers\": \"dim_voucher\",\n","    \"orders\": \"fact_orders\",\n","    \"product_category\" : \"dim_product_category\",\n","    \"products\" : \"dim_products\",\n","    \"order_items\" : \"fact_order_items\"\n","}\n","\n","# data OLTP yang ada tabel user, connect-kan di DWH dim_user. 2 tabel salig berhubungan\n","# payments di OLTP, klo di DWH itu dim_payment\n","# tipe datanya adalah dictionary agar logiccodingannya lebih mudah\n","\n","# pastikan nama-nama kolomnya benar !\n","\n","# PERUBAHAN\n","# 1. kolom user_birthday di dim_users diubah menjadi user_birthdate\n","# 2. menambahkan kolom dim_product_category, dim_products, dan fact_order_items\n","\n","dimension_columns = {\n","    \"dim_users\": [\"user_id\", \"user_first_name\", \"user_last_name\", \"user_gender\", \"user_address\", \"user_birthdate\", \"user_join\"],\n","    \"dim_payments\": [\"payment_id\", \"payment_name\", \"payment_status\"],\n","    \"dim_shippers\": [\"shipper_id\", \"shipper_name\"],\n","    \"dim_ratings\": [\"rating_id\", \"rating_level\", \"rating_status\"],\n","    \"dim_voucher\": [\"voucher_id\", \"voucher_name\", \"voucher_price\", \"voucher_created\",\"user_id\"],\n","    \"fact_orders\": ['order_id', 'order_date', 'user_id', 'payment_id', 'shipper_id', 'order_price','order_discount', 'voucher_id', 'order_total', 'rating_id'],\n","    \"dim_product_category\" : [ \"product_category_id\", \"product_category_name\" ],\n","    \"dim_products\" : [ \"product_id\", \"product_category_id\", \"product_name\", \"product_created\", \"product_price\", \"product_discount\"],\n","    \"fact_order_items\" : [ 'order_item_id',  'order_id', 'product_id', 'order_item_quantity', 'product_discount', 'product_subdiscount', 'product_price', 'product_subprice' ]\n","\n","    }\n","\n","\n","# perubahan sesuai Query DDL\n","ddl_statements = {\n","    \"dim_users\": \"\"\"\n","       CREATE TABLE IF NOT EXISTS dim_users (\n","            user_id INT NOT NULL PRIMARY KEY,\n","            user_first_name VARCHAR(255) NOT NULL,\n","            user_last_name VARCHAR(255) NOT NULL,\n","            user_gender VARCHAR(50) NOT NULL,\n","            user_address VARCHAR(255),\n","            user_birthdate DATE NOT NULL,\n","            user_join DATE NOT NULL\n","        );\n","    \"\"\",\n","    \"dim_payments\": \"\"\"\n","        CREATE TABLE IF NOT EXISTS dim_payments (\n","            payment_id INT NOT NULL PRIMARY KEY,\n","            payment_name VARCHAR(255) NOT NULL,\n","            payment_status BOOLEAN NOT NULL\n","        );\n","    \"\"\",\n","    \"dim_shippers\": \"\"\"\n","        CREATE TABLE IF NOT EXISTS dim_shippers (\n","            shipper_id INT NOT NULL PRIMARY KEY,\n","            shipper_name VARCHAR(255) NOT NULL\n","        );\n","    \"\"\",\n","    \"dim_ratings\": \"\"\"\n","       CREATE TABLE IF NOT EXISTS dim_ratings (\n","            rating_id INT NOT NULL PRIMARY KEY,\n","            rating_level INT NOT NULL,\n","            rating_status VARCHAR(255) NOT NULL\n","        );\n","    \"\"\",\n","    \"dim_voucher\": \"\"\"\n","       CREATE TABLE IF NOT EXISTS dim_voucher (\n","            voucher_id INT NOT NULL PRIMARY KEY,\n","            voucher_name VARCHAR(255) NOT NULL,\n","            voucher_price INT NOT NULL,\n","            voucher_created DATE NOT NULL,\n","            user_id INT NOT NULL\n","        );\n","    \"\"\",\n","    \"fact_orders\": \"\"\"\n","       CREATE TABLE IF NOT EXISTS fact_orders (\n","            order_id INT NOT NULL PRIMARY KEY,\n","            order_date DATE NOT NULL,\n","            user_id INT NOT NULL,\n","            payment_id INT NOT NULL,\n","            shipper_id INT NOT NULL,\n","            order_price INT NOT NULL,\n","            order_discount INT,\n","            voucher_id INT NOT NULL,\n","            order_total INT NOT NULL,\n","            rating_id INT NOT NULL,\n","            FOREIGN KEY (user_id) REFERENCES dim_users(user_id),\n","            FOREIGN KEY (payment_id) REFERENCES dim_payments(payment_id),\n","            FOREIGN KEY (shipper_id) REFERENCES dim_shippers(shipper_id),\n","            FOREIGN KEY (voucher_id) REFERENCES dim_voucher(voucher_id),\n","            FOREIGN KEY (rating_id) REFERENCES dim_ratings(rating_id)\n","        );\n","    \"\"\",\n","     \"dim_product_category\": \"\"\"\n","      CREATE TABLE IF NOT EXISTS dim_product_category (\n","            product_category_id INT NOT NULL PRIMARY KEY,\n","            product_category_name VARCHAR(255) NOT NULL\n","        );\n","    \"\"\",\n","     \"dim_products\": \"\"\"\n","      CREATE TABLE IF NOT EXISTS dim_products (\n","            product_id INT NOT NULL PRIMARY KEY,\n","            product_category_id INT NOT NULL,\n","            product_name VARCHAR(255) NOT NULL,\n","            product_created DATE NOT NULL,\n","            product_price INT NOT NULL,\n","            product_discount INT,\n","            FOREIGN KEY (product_category_id) REFERENCES dim_product_category(product_category_id)\n","        );\n","    \"\"\",\n","     \"fact_order_items\": \"\"\"\n","      CREATE TABLE IF NOT EXISTS fact_order_items (\n","            order_item_id INT NOT NULL PRIMARY KEY,\n","            order_id INT NOT NULL,\n","            product_id INT NOT NULL,\n","            order_item_quantity INT,\n","            product_discount INT,\n","            product_subdiscount INT,\n","            product_price INT NOT NULL,\n","            product_subprice INT NOT NULL,\n","            FOREIGN KEY (order_id) REFERENCES fact_orders(order_id),\n","            FOREIGN KEY (product_id) REFERENCES dim_products(product_id)\n","        );\n","    \"\"\"\n","\n","}"],"metadata":{"id":"YvrJQkRnHMtW"},"id":"YvrJQkRnHMtW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. ) Data Marts ( dm_sales )"],"metadata":{"id":"jWFmnRYlMFa4"},"id":"jWFmnRYlMFa4"},{"cell_type":"code","source":["ddl_marts_1 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_sales (\n","           month_sales CHAR(7) NOT NULL,\n","           total_sales INT NOT NULL,\n","           PRIMARY KEY (month_sales)\n","       );\n","\n","      INSERT INTO dm_sales (month_sales, total_sales )\n","        SELECT\n","          TO_CHAR(order_date, 'MM') AS month_sales,\n","          SUM(oi.order_item_quantity * oi.product_price) AS total_sales\n","        FROM\n","          fact_orders o\n","        JOIN\n","          fact_order_items oi ON o.order_id = oi.order_id\n","        GROUP BY\n","          TO_CHAR(order_date, 'MM')\n","        ORDER BY\n","          month_sales;\n","    \"\"\"\n","}"],"metadata":{"id":"KkgtMIrLL69o","executionInfo":{"status":"ok","timestamp":1716715827946,"user_tz":-180,"elapsed":3,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}}},"id":"KkgtMIrLL69o","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## 2.) Data Marts ( dm_product_sales_2 )"],"metadata":{"id":"vkHX6RtOM3NF"},"id":"vkHX6RtOM3NF"},{"cell_type":"code","source":["ddl_marts_2 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_product_sales_2 (\n","              product_category_id INT NOT NULL,\n","              product_category_name VARCHAR(255) NOT NULL,\n","              total_sales INT NOT NULL,\n","              PRIMARY KEY (product_category_id),\n","              FOREIGN KEY (product_category_id) REFERENCES dim_product_category(product_category_id)\n","            );\n","\n","\n","        INSERT INTO dm_product_sales_2 (product_category_id, product_category_name, total_sales)\n","        SELECT\n","          p.product_category_id,\n","          c.product_category_name,\n","          SUM(oi.order_item_quantity * oi.product_price) AS total_sales\n","        FROM\n","          dim_products p\n","        JOIN\n","          dim_product_category c ON p.product_category_id = c.product_category_id\n","        JOIN\n","          fact_order_items oi ON p.product_id = oi.product_id\n","        GROUP BY\n","          p.product_category_id, c.product_category_name;\n","    \"\"\"\n","}"],"metadata":{"id":"9pnqV3J3M3AD"},"id":"9pnqV3J3M3AD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.) Data Marts ( dm_payment_sales_3 )"],"metadata":{"id":"mlVfWbW7NNyr"},"id":"mlVfWbW7NNyr"},{"cell_type":"code","source":["ddl_marts_3 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_payment_sales_3 (\n","            payment_id INT NOT NULL,\n","            payment_name VARCHAR(255) NOT NULL,\n","            total_sales INT NOT NULL,\n","            PRIMARY KEY (payment_id),\n","            FOREIGN KEY (payment_id) REFERENCES dim_payments(payment_id)\n","          );\n","\n","\n","        INSERT INTO dm_payment_sales_3 (payment_id, payment_name, total_sales)\n","        SELECT\n","          o.payment_id,\n","          p.payment_name,\n","          SUM(o.order_total) AS total_sales\n","        FROM\n","          fact_orders o\n","        JOIN\n","          dim_payments p ON o.payment_id = p.payment_id\n","        GROUP BY\n","          o.payment_id, p.payment_name;\n","    \"\"\"\n","}"],"metadata":{"id":"PUNO2_CsNNnd"},"id":"PUNO2_CsNNnd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.) Data Marts ( dm_shipper_sales_4 )"],"metadata":{"id":"c2borqBdNc6-"},"id":"c2borqBdNc6-"},{"cell_type":"code","source":["ddl_marts_4 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_shipper_sales_4 (\n","            shipper_id INT NOT NULL,\n","            shipper_name VARCHAR(255) NOT NULL,\n","            total_sales INT NOT NULL,\n","            PRIMARY KEY (shipper_id),\n","            FOREIGN KEY (shipper_id) REFERENCES dim_shippers(shipper_id)\n","          );\n","\n","        INSERT INTO dm_shipper_sales_4 (shipper_id, shipper_name, total_sales)\n","        SELECT\n","          o.shipper_id,\n","          s.shipper_name,\n","          SUM(o.order_total) AS total_sales\n","        FROM\n","          fact_orders o\n","        JOIN\n","          dim_shippers s ON o.shipper_id = s.shipper_id\n","        GROUP BY\n","          o.shipper_id, s.shipper_name;\n","\n","    \"\"\"\n","}"],"metadata":{"id":"a_dMHtkiNczD"},"id":"a_dMHtkiNczD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.) Data Marts ( dm_user_sales_5  )"],"metadata":{"id":"IJlhkjrNNtQj"},"id":"IJlhkjrNNtQj"},{"cell_type":"code","source":["ddl_marts_5 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_user_sales_5 (\n","            user_id INT NOT NULL,\n","            user_first_name VARCHAR(255) NOT NULL,\n","            user_last_name VARCHAR(255) NOT NULL,\n","            total_sales INT NOT NULL,\n","            PRIMARY KEY (user_id),\n","            FOREIGN KEY (user_id) REFERENCES dim_users(user_id)\n","          );\n","\n","\n","        INSERT INTO dm_user_sales_5 (user_id, user_first_name, user_last_name, total_sales)\n","        SELECT\n","          u.user_id,\n","          u.user_first_name,\n","          u.user_last_name,\n","          SUM(o.order_total) AS total_sales\n","        FROM\n","          dim_users u\n","        JOIN\n","          fact_orders o ON u.user_id = o.user_id\n","        GROUP BY\n","          u.user_id, u.user_first_name, u.user_last_name;\n","\n","    \"\"\"\n","}"],"metadata":{"id":"PseSmh7UNtGl"},"id":"PseSmh7UNtGl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.) Data Marts ( dm_discount_voucher_trend_6 )"],"metadata":{"id":"bFxShUuoN6_v"},"id":"bFxShUuoN6_v"},{"cell_type":"code","source":["ddl_marts_6 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_discount_voucher_trend_6 (\n","            trend_id SERIAL PRIMARY KEY,\n","            trend_date DATE NOT NULL,\n","            discount_amount INT NOT NULL,\n","            voucher_amount INT NOT NULL\n","          );\n","\n","\n","        INSERT INTO dm_discount_voucher_trend_6 (trend_date, discount_amount, voucher_amount)\n","        SELECT\n","          DATE_TRUNC('month', o.order_date) AS trend_date,\n","          SUM(o.order_discount) AS discount_amount,\n","          SUM(voucher.voucher_price) AS voucher_amount\n","        FROM\n","          fact_orders o\n","        LEFT JOIN\n","          dim_voucher voucher ON o.voucher_id = voucher.voucher_id\n","        GROUP BY\n","          trend_date;\n","\n","    \"\"\"\n","}"],"metadata":{"id":"PK3eZuGqN62x"},"id":"PK3eZuGqN62x","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7.) Data Marts ( dm_sales_performance_by_region_7 )"],"metadata":{"id":"qg9kmOdJONKA"},"id":"qg9kmOdJONKA"},{"cell_type":"code","source":["ddl_marts_7 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_sales_performance_by_region_7 (\n","            region VARCHAR(255) NOT NULL,\n","            total_sales INT NOT NULL,\n","            PRIMARY KEY (region)\n","          );\n","\n","        INSERT INTO dm_sales_performance_by_region_7 (region, total_sales)\n","        SELECT\n","          u.user_address AS region,\n","          SUM(o.order_total) AS total_sales\n","        FROM\n","          fact_orders o\n","        JOIN\n","          dim_users u ON o.user_id = u.user_id\n","        GROUP BY\n","          u.user_address;\n","\n","    \"\"\"\n","}"],"metadata":{"id":"2L6x19HrONAZ"},"id":"2L6x19HrONAZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8.) Data Marts ( dm_profit_margin_per_category_8 )"],"metadata":{"id":"G0qe0x73OcwA"},"id":"G0qe0x73OcwA"},{"cell_type":"code","source":["ddl_marts_8 = {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_profit_margin_per_category_8 (\n","            category_id INT NOT NULL PRIMARY KEY,\n","            category_name VARCHAR(255) NOT NULL,\n","            total_sales INT NOT NULL,\n","            total_cost INT NOT NULL,\n","            total_profit INT NOT NULL\n","          );\n","\n","        INSERT INTO dm_profit_margin_per_category_8 (category_id, category_name, total_sales, total_cost, total_profit)\n","        SELECT\n","          pc.product_category_id AS category_id,\n","          pc.product_category_name AS category_name,\n","          SUM(oi.product_price * oi.order_item_quantity) AS total_sales,\n","          SUM(oi.product_subprice * oi.order_item_quantity) AS total_cost,\n","          SUM((oi.product_price - oi.product_subprice) * oi.order_item_quantity) AS total_profit\n","        FROM\n","          fact_order_items oi\n","        JOIN\n","          dim_products p ON oi.product_id = p.product_id\n","        JOIN\n","          dim_product_category pc ON p.product_category_id = pc.product_category_id\n","        GROUP BY\n","          pc.product_category_id, pc.product_category_name;\n","\n","    \"\"\"\n","}"],"metadata":{"id":"ZOKmVe8fOcmI"},"id":"ZOKmVe8fOcmI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9.) Data Marts ( dm_average_order_value_per_user_9 )"],"metadata":{"id":"83LmYb_pOtNI"},"id":"83LmYb_pOtNI"},{"cell_type":"code","source":["ddl_marts_9= {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_average_order_value_per_user_9 (\n","            user_id INT NOT NULL PRIMARY KEY,\n","            user_first_name VARCHAR(255) NOT NULL,\n","            user_last_name VARCHAR(255) NOT NULL,\n","            average_order_value NUMERIC(10, 2) NOT NULL\n","          );\n","\n","        INSERT INTO dm_average_order_value_per_user_9 (user_id, user_first_name, user_last_name, average_order_value)\n","        SELECT\n","          u.user_id,\n","          u.user_first_name,\n","          u.user_last_name,\n","          AVG(fo.order_total) AS average_order_value\n","        FROM\n","          dim_users u\n","        JOIN\n","          fact_orders fo ON u.user_id = fo.user_id\n","        GROUP BY\n","          u.user_id, u.user_first_name, u.user_last_name;\n","\n","    \"\"\"\n","}"],"metadata":{"id":"lJZ0BA0hOtED"},"id":"lJZ0BA0hOtED","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10.) Data Marts ( dm_voucher_conversion_rate_10  )"],"metadata":{"id":"vN4yBw4jO7Tm"},"id":"vN4yBw4jO7Tm"},{"cell_type":"code","source":["ddl_marts_9= {\n","    \"\"\"\n","       CREATE TABLE IF NOT EXISTS dm_voucher_conversion_rate_10 (\n","            voucher_id INT NOT NULL PRIMARY KEY,\n","            voucher_name VARCHAR(255) NOT NULL,\n","            total_orders_with_voucher INT NOT NULL,\n","            total_orders_without_voucher INT NOT NULL,\n","            conversion_rate NUMERIC(5, 2) NOT NULL\n","          );\n","\n","\n","        INSERT INTO dm_voucher_conversion_rate_10 (voucher_id, voucher_name, total_orders_with_voucher, total_orders_without_voucher, conversion_rate)\n","        SELECT\n","          v.voucher_id,\n","          v.voucher_name,\n","          COALESCE(COUNT(fo.order_id), 0) AS total_orders_with_voucher,\n","          (SELECT COUNT(*) FROM fact_orders WHERE voucher_id IS NULL) AS total_orders_without_voucher,\n","          CASE\n","              WHEN COUNT(fo.order_id) > 0 THEN (COUNT(fo.order_id)::NUMERIC / (COUNT(fo.order_id) + (SELECT COUNT(*) FROM fact_orders WHERE voucher_id IS NULL))::NUMERIC) * 100\n","              ELSE 0\n","          END AS conversion_rate\n","        FROM\n","          dim_voucher v\n","        LEFT JOIN\n","          fact_orders fo ON v.voucher_id = fo.voucher_id\n","        GROUP BY\n","          v.voucher_id, v.voucher_name;\n","\n","\n","    \"\"\"\n","}"],"metadata":{"id":"u3FBV6KAO7KD"},"id":"u3FBV6KAO7KD","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"817cfd59-a54b-4394-9656-9d0e79711d0c","metadata":{"id":"817cfd59-a54b-4394-9656-9d0e79711d0c"},"source":["# **Load Library**"]},{"cell_type":"code","execution_count":null,"id":"3dc69a03-0055-4ae8-91d9-52ebb078f872","metadata":{"tags":[],"id":"3dc69a03-0055-4ae8-91d9-52ebb078f872"},"outputs":[],"source":["import pandas as pd\n","import sqlalchemy as sa"]},{"cell_type":"markdown","id":"86f36d37-dd56-4c5f-8690-dd88bc6f510f","metadata":{"id":"86f36d37-dd56-4c5f-8690-dd88bc6f510f"},"source":["# **Function ETL**"]},{"cell_type":"markdown","source":["### create table"],"metadata":{"id":"-5dZ81In0Hot"},"id":"-5dZ81In0Hot"},{"cell_type":"code","source":["# Buat tabel DWH fact dan dim nya\n","def create_tables():\n","    \"\"\"Create tables in the data warehouse if they do not exist.\"\"\"\n","    #Menjelaskan bahwa fungsi ini bertugas untuk membuat tabel di data warehouse jika tabel-tabel tersebut belum ada.\n","\n","    engine = sa.create_engine(warehouse_conn_string) # create table baru\n","    #sa.create_engine(warehouse_conn_string): Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy)\n","    #untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn: # tes koneksi\n","    #with engine.connect() as conn: Membuka koneksi ke database menggunakan engine yang baru dibuat.\n","    #Blok with memastikan bahwa koneksi akan ditutup secara otomatis setelah blok selesai dieksekusi.\n","    #conn: Objek koneksi yang digunakan untuk berinteraksi dengan database.\n","\n","        for ddl in ddl_statements.values():\n","          # ddl_statements: Koleksi dari pernyataan DDL (Data Definition Language) yang berisi SQL untuk membuat tabel.\n","          # Ini bisa berupa dictionary di mana setiap nilai adalah pernyataan SQL.\n","\n","          # for ddl in ddl_statements.values(): Melakukan iterasi melalui setiap pernyataan DDL dalam koleksi.\n","\n","            conn.execute(ddl) # di running tablenya\n","            # conn.execute(ddl): Menjalankan pernyataan DDL menggunakan koneksi yang telah dibuka. Ini akan membuat tabel di database data warehouse sesuai dengan pernyataan SQL yang ada dalam ddl_statements."],"metadata":{"id":"Dqy5sV1_z1mJ"},"id":"Dqy5sV1_z1mJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Potongan kode ini memastikan bahwa semua tabel yang diperlukan di data warehouse dibuat jika belum ada, mempersiapkan data warehouse untuk proses ETL selanjutnya.\n","\n","Fungsi create_tables bertujuan untuk membuat tabel di data warehouse jika tabel-tabel tersebut belum ada. Fungsi ini bekerja dengan cara:\n","\n","1. Membuat engine untuk koneksi ke data warehouse menggunakan string koneksi.\n","2. Membuka koneksi ke database.\n","3. Melakukan iterasi melalui sekumpulan pernyataan DDL.\n","4. Menjalankan setiap pernyataan DDL untuk membuat tabel yang diperlukan."],"metadata":{"id":"vMOEV8tSo9U5"},"id":"vMOEV8tSo9U5"},{"cell_type":"code","source":["# Check dengan running\n","for i in ddl_statements:\n","  print(i)"],"metadata":{"id":"JiZ3LkfXz4QG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697433906,"user_tz":-180,"elapsed":299,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"a0b72c38-1755-4e2e-ef9f-6b9c6cbb7e47"},"id":"JiZ3LkfXz4QG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dim_users\n","dim_payments\n","dim_shippers\n","dim_ratings\n","dim_voucher\n","fact_orders\n","dim_product_category\n","dim_products\n","fact_order_items\n"]}]},{"cell_type":"markdown","source":["### ekstrak data"],"metadata":{"id":"fRT9iSue0Dw_"},"id":"fRT9iSue0Dw_"},{"cell_type":"code","source":["# Cek OLTP tables\n","oltp_tables"],"metadata":{"id":"1c0Kgk3P0Qt8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697508581,"user_tz":-180,"elapsed":308,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"ea6eb3cc-9f4f-4823-93a1-9a073398c2d3"},"id":"1c0Kgk3P0Qt8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'users': 'tb_users',\n"," 'payments': 'tb_payments',\n"," 'shippers': 'tb_shippers',\n"," 'ratings': 'tb_ratings',\n"," 'vouchers': 'tb_voucher',\n"," 'orders': 'tb_orders',\n"," 'product_category': 'tb_product_category',\n"," 'products': 'tb_products',\n"," 'order_items': 'tb_order_items'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# OLTP tables kolom 'user'\n","oltp_tables['users']"],"metadata":{"id":"oCNpUlCR0Wd9","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1716697511696,"user_tz":-180,"elapsed":304,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"5a933b57-0a7d-4702-dc3c-62e3740565da"},"id":"oCNpUlCR0Wd9","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'tb_users'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Cek warehouse tables\n","warehouse_tables"],"metadata":{"id":"iGhoxS1x0ewc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697514364,"user_tz":-180,"elapsed":329,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"be922eeb-2400-4e6c-fa7f-3da72a4b4fd4"},"id":"iGhoxS1x0ewc","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'users': 'dim_users',\n"," 'payments': 'dim_payments',\n"," 'shippers': 'dim_shippers',\n"," 'ratings': 'dim_ratings',\n"," 'vouchers': 'dim_voucher',\n"," 'orders': 'fact_orders',\n"," 'product_category': 'dim_product_category',\n"," 'products': 'dim_products',\n"," 'order_items': 'fact_order_items'}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# warehouse tables kolom 'user'\n","warehouse_tables['users']"],"metadata":{"id":"XbgeI70c0lT1","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1716697518596,"user_tz":-180,"elapsed":295,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"2ab26235-e3e9-443c-bcf6-6386924beb4b"},"id":"XbgeI70c0lT1","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'dim_users'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def extract_data(table_name):\n","  # Menjelaskan bahwa fungsi ini bertugas untuk mengekstrak data dari sebuah tabel di database OLTP (Online Transaction Processing).\n","    \"\"\"Extract data from a table in the OLTP database.\"\"\"\n","    engine = sa.create_engine(oltp_conn_string)\n","    # sa.create_engine(oltp_conn_string): Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database OLTP.\n","    # oltp_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database OLTP.\n","\n","\n","    query = f\"SELECT * FROM {oltp_tables[table_name]}\"\n","    # Membuat Query SQL: Menggunakan f-string untuk membentuk query SQL yang akan mengekstrak semua data (SELECT *) dari tabel yang namanya diambil dari oltp_tables[table_name].\n","    # oltp_tables: Diasumsikan sebagai dictionary atau mapping yang berisi nama-nama tabel di database OLTP. table_name adalah kunci untuk mendapatkan nama tabel yang benar.\n","\n","    df = pd.read_sql(query, engine)\n","    # pd.read_sql(query, engine): Menggunakan fungsi read_sql dari Pandas (pd) untuk menjalankan query SQL dan mengembalikan hasilnya sebagai DataFrame (df).\n","    # query adalah query SQL yang telah dibuat, dan engine adalah engine SQLAlchemy yang digunakan untuk koneksi ke database.\n","\n","    print(f'Extract Data {oltp_tables[table_name]} Success')\n","    # Mencetak Pesan: Menggunakan f-string untuk mencetak pesan sukses yang menyatakan bahwa data dari tabel yang dimaksud telah berhasil diekstraksi.\n","    return df\n","    # Mengembalikan DataFrame: Mengembalikan DataFrame (df) yang berisi data yang diekstraksi dari tabel di database OLTP."],"metadata":{"id":"F8Q57zIQ0Fip"},"id":"F8Q57zIQ0Fip","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fungsi extract_data bertujuan untuk mengekstrak data dari tabel tertentu di database OLTP dan mengembalikannya sebagai DataFrame Pandas. Berikut adalah langkah-langkah yang dilakukan oleh fungsi ini:\n","\n","1. Membuat Engine Koneksi: Membuat engine koneksi ke database OLTP menggunakan string koneksi.\n","2. Membuat Query SQL: Membuat query SQL untuk memilih semua data dari tabel yang ditentukan.\n","3. Menjalankan Query: Menjalankan query menggunakan Pandas dan menyimpan hasilnya dalam DataFrame.\n","4. Mencetak Pesan Sukses: Mencetak pesan yang menunjukkan bahwa data telah berhasil diekstraksi.\n","5. Mengembalikan Data: Mengembalikan DataFrame yang berisi data yang diekstraksi.\n","\n","Fungsi ini adalah bagian penting dari proses ETL (Extract, Transform, Load) yang bertugas untuk mengekstrak data dari sumber OLTP sebelum data tersebut ditransformasi dan dimuat ke dalam data warehouse."],"metadata":{"id":"6tv3zdIipFP9"},"id":"6tv3zdIipFP9"},{"cell_type":"markdown","source":["### transform data"],"metadata":{"id":"Hu1JoHH21R1d"},"id":"Hu1JoHH21R1d"},{"cell_type":"markdown","source":["#### a.) Dimensional"],"metadata":{"id":"9rvEDL0iaDRJ"},"id":"9rvEDL0iaDRJ"},{"cell_type":"code","source":["def transform_data(df, target_table):\n","    \"\"\"Transform the extracted data to match the schema of the target dimension table.\"\"\"\n","    # Menjelaskan bahwa fungsi ini bertujuan untuk mentransformasi data yang diekstrak agar sesuai dengan skema tabel dimensi target.\n","\n","    columns = dimension_columns.get(target_table)\n","    # dimension_columns.get(target_table): Mengambil daftar kolom yang diperlukan untuk tabel dimensi target dari dimension_columns,\n","    # yang diasumsikan sebagai dictionary yang memetakan nama tabel ke daftar kolom yang dibutuhkan.\n","\n","    if columns:\n","    # if columns:: Memeriksa apakah kolom yang dibutuhkan tersedia (tidak None).\n","\n","        df = df[columns]\n","        # df = df[columns]: Jika kolom tersedia, menyaring DataFrame (df) agar hanya berisi kolom yang ditentukan dalam columns.\n","        # Ini memastikan bahwa DataFrame yang dikembalikan hanya memiliki kolom yang sesuai dengan skema tabel dimensi target.\n","\n","    print(f'Transform Data {target_table} Success')\n","    # Mencetak Pesan: Menggunakan f-string untuk mencetak pesan sukses yang menunjukkan bahwa data untuk tabel dimensi target telah berhasil ditransformasi.\n","\n","    return df\n","    # Mengembalikan DataFrame: Mengembalikan DataFrame (df) yang telah ditransformasi agar sesuai dengan skema tabel dimensi target."],"metadata":{"id":"JGmh45_x1WSI"},"id":"JGmh45_x1WSI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fungsi ini merupakan bagian penting dari proses ETL (Extract, Transform, Load) yang bertugas untuk mentransformasi data setelah diekstrak dan sebelum dimuat ke dalam tabel dimensi di data warehouse.\n","\n","Fungsi transform_data bertujuan untuk mentransformasi data yang diekstrak agar sesuai dengan skema tabel dimensi target. Berikut adalah langkah-langkah yang dilakukan oleh fungsi ini:\n","\n","1. Mengambil Kolom yang Dibutuhkan: Mengambil daftar kolom yang diperlukan untuk tabel dimensi target dari dimension_columns.\n","2. Menyaring Kolom DataFrame: Jika kolom yang dibutuhkan tersedia, menyaring DataFrame agar hanya berisi kolom yang sesuai dengan skema tabel dimensi target.\n","3. Mencetak Pesan Sukses: Mencetak pesan yang menunjukkan bahwa data telah berhasil ditransformasi.\n","4. Mengembalikan Data: Mengembalikan DataFrame yang berisi data yang telah ditransformasi."],"metadata":{"id":"B-WvpLgtpWob"},"id":"B-WvpLgtpWob"},{"cell_type":"markdown","source":["Jadi, transform yang transform_data untuk yang dimensional, sedangkan yang dibawah, yang transform_fact_orders untuk yang fact table aja"],"metadata":{"id":"emuGX0-r1hhD"},"id":"emuGX0-r1hhD"},{"cell_type":"markdown","source":["#### b.) fact_orders"],"metadata":{"id":"zCKSbXmVaG31"},"id":"zCKSbXmVaG31"},{"cell_type":"code","source":["def transform_fact_orders():\n","    \"\"\"Transform data for the fact_orders table.\"\"\"\n","    # Menjelaskan bahwa fungsi ini bertujuan untuk mentransformasi data yang diperlukan untuk tabel fact_orders.\n","\n","    dataframes = {table: extract_data(table) for table in oltp_tables.keys()}\n","    # {table: extract_data(table) for table in oltp_tables.keys()}: Membuat dictionary dataframes yang berisi DataFrame hasil ekstraksi data dari setiap tabel dalam oltp_tables.\n","    # Ini menggunakan dictionary comprehension untuk mengiterasi melalui kunci-kunci tabel dalam oltp_tables dan mengekstrak data untuk setiap tabel menggunakan fungsi extract_data\n","\n","    df_orders = dataframes['orders']\n","    df_orders = df_orders.merge(dataframes['users'], on='user_id')\n","    df_orders = df_orders.merge(dataframes['payments'], on='payment_id')\n","    df_orders = df_orders.merge(dataframes['shippers'], on='shipper_id')\n","    df_orders = df_orders.merge(dataframes['ratings'], on='rating_id')\n","    df_orders = df_orders.merge(dataframes['vouchers'], how='left', on='voucher_id')\n","    # Penggabungan Data: DataFrame df_orders digabungkan dengan DataFrame lain yang diekstrak sebelumnya. Ini termasuk tabel users, payments, shippers, ratings, dan vouchers.\n","    # Penggabungan dilakukan berdasarkan kolom-kolom yang sesuai seperti user_id, payment_id, shipper_id, rating_id, dan voucher_id.\n","\n","\n","    df_orders.rename(columns={'user_id_x': 'user_id'}, inplace=True)\n","    # Mengganti Nama Kolom: Karena setelah penggabungan terdapat kolom-kolom yang berasal dari tabel yang sama, perlu dilakukan penamaan ulang agar tidak terjadi duplikasi nama kolom.\n","    # Di sini, kolom yang berasal dari users diubah namanya dari user_id_x menjadi user_id.\n","\n","    fact_orders_columns = dimension_columns.get('fact_orders') # karena tabel targetnya adalah afct order\n","    return df_orders[fact_orders_columns]\n","    # Memilih Kolom Tertentu: Mengambil kolom-kolom yang sesuai dengan skema tabel fact_orders dari DataFrame df_orders.\n","    # Kolom-kolom yang dipilih sesuai dengan yang telah ditentukan sebelumnya dalam dimension_columns.\n"],"metadata":{"id":"1jYmgPhg1hCQ"},"id":"1jYmgPhg1hCQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fungsi transform_fact_orders adalah bagian dari proses ETL yang bertugas untuk mentransformasi data yang diekstrak dari tabel-tabel sumber menjadi format yang sesuai dengan skema tabel fact_orders dalam data warehouse. Langkah-langkah yang dilakukan termasuk penggabungan data dari berbagai tabel sumber, penggantian nama kolom, dan pemilihan kolom tertentu sesuai dengan skema tabel target.\n","\n","Jadi, fungsi ini melakukan transformasi data yang diperlukan untuk tabel fact_orders sehingga data tersebut siap untuk dimuat ke dalam data warehouse."],"metadata":{"id":"25bU47lWphOq"},"id":"25bU47lWphOq"},{"cell_type":"code","source":["# ngecek tipe data\n","dimension_columns"],"metadata":{"id":"xRzyolKT5bJs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697716138,"user_tz":-180,"elapsed":298,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"bf0f1960-7f84-4486-86ed-9ea7f1f1e441"},"id":"xRzyolKT5bJs","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dim_users': ['user_id',\n","  'user_first_name',\n","  'user_last_name',\n","  'user_gender',\n","  'user_address',\n","  'user_birthdate',\n","  'user_join'],\n"," 'dim_payments': ['payment_id', 'payment_name', 'payment_status'],\n"," 'dim_shippers': ['shipper_id', 'shipper_name'],\n"," 'dim_ratings': ['rating_id', 'rating_level', 'rating_status'],\n"," 'dim_voucher': ['voucher_id',\n","  'voucher_name',\n","  'voucher_price',\n","  'voucher_created',\n","  'user_id'],\n"," 'fact_orders': ['order_id',\n","  'order_date',\n","  'user_id',\n","  'payment_id',\n","  'shipper_id',\n","  'order_price',\n","  'order_discount',\n","  'voucher_id',\n","  'order_total',\n","  'rating_id'],\n"," 'dim_product_category': ['product_category_id', 'product_category_name'],\n"," 'dim_products': ['product_id',\n","  'product_category_id',\n","  'product_name',\n","  'product_created',\n","  'product_price',\n","  'product_discount'],\n"," 'fact_order_items': ['order_item_id',\n","  'order_id',\n","  'product_id',\n","  'order_item_quantity',\n","  'product_discount',\n","  'product_subdiscount',\n","  'product_price',\n","  'product_subprice']}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["misal kita mau ambil isi dari dim_users"],"metadata":{"id":"IszfD6tw18MS"},"id":"IszfD6tw18MS"},{"cell_type":"code","source":["dimension_columns['dim_users']"],"metadata":{"id":"mNgfYBEq11hN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697722809,"user_tz":-180,"elapsed":419,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"0c5d641d-eb2b-41d3-c5ad-3b014c8500b0"},"id":"mNgfYBEq11hN","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['user_id',\n"," 'user_first_name',\n"," 'user_last_name',\n"," 'user_gender',\n"," 'user_address',\n"," 'user_birthdate',\n"," 'user_join']"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["fact_orders_columns = dimension_columns.get('fact_orders')\n","fact_orders_columns"],"metadata":{"id":"oclnUbkc2a4-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697728946,"user_tz":-180,"elapsed":303,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"6c834901-5077-4eaf-a80e-b894f0d8d51a"},"id":"oclnUbkc2a4-","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['order_id',\n"," 'order_date',\n"," 'user_id',\n"," 'payment_id',\n"," 'shipper_id',\n"," 'order_price',\n"," 'order_discount',\n"," 'voucher_id',\n"," 'order_total',\n"," 'rating_id']"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["#### c.) fact_order_items"],"metadata":{"id":"ZJFTXcHuaL7y"},"id":"ZJFTXcHuaL7y"},{"cell_type":"code","source":["def transform_fact_order_items():\n","    \"\"\"Transform data for the fact_orders table.\"\"\"\n","    dataframes = {table: extract_data(table) for table in oltp_tables.keys()}\n","\n","    df_order_items = dataframes['order_items']\n","    df_order_items = df_orders.merge(dataframes['product_category'], on='product_category_id')\n","    df_order_items = df_orders.merge(dataframes['products'], on='product_id')\n","\n","    fact_order_items_columns = dimension_columns.get('fact_order_items') # karena tabel targetnya adalah afct order\n","    return df_order_items[fact_order_items_columns]\n"],"metadata":{"id":"G7K_KzESaUtT"},"id":"G7K_KzESaUtT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ngecek tipe data\n","dimension_columns"],"metadata":{"id":"emHVzbT9bzHg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697742013,"user_tz":-180,"elapsed":325,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"2cfb8872-03c5-4f1d-8ee0-ced3e8e4ad6e"},"id":"emHVzbT9bzHg","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dim_users': ['user_id',\n","  'user_first_name',\n","  'user_last_name',\n","  'user_gender',\n","  'user_address',\n","  'user_birthdate',\n","  'user_join'],\n"," 'dim_payments': ['payment_id', 'payment_name', 'payment_status'],\n"," 'dim_shippers': ['shipper_id', 'shipper_name'],\n"," 'dim_ratings': ['rating_id', 'rating_level', 'rating_status'],\n"," 'dim_voucher': ['voucher_id',\n","  'voucher_name',\n","  'voucher_price',\n","  'voucher_created',\n","  'user_id'],\n"," 'fact_orders': ['order_id',\n","  'order_date',\n","  'user_id',\n","  'payment_id',\n","  'shipper_id',\n","  'order_price',\n","  'order_discount',\n","  'voucher_id',\n","  'order_total',\n","  'rating_id'],\n"," 'dim_product_category': ['product_category_id', 'product_category_name'],\n"," 'dim_products': ['product_id',\n","  'product_category_id',\n","  'product_name',\n","  'product_created',\n","  'product_price',\n","  'product_discount'],\n"," 'fact_order_items': ['order_item_id',\n","  'order_id',\n","  'product_id',\n","  'order_item_quantity',\n","  'product_discount',\n","  'product_subdiscount',\n","  'product_price',\n","  'product_subprice']}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["dimension_columns['dim_product_category']"],"metadata":{"id":"n3EJFeaIb1DN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697749823,"user_tz":-180,"elapsed":315,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"75d9ee62-2219-4b0f-af8c-a20e5f80da51"},"id":"n3EJFeaIb1DN","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['product_category_id', 'product_category_name']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["fact_order_items_columns = dimension_columns.get('fact_order_items')\n","fact_order_items_columns"],"metadata":{"id":"iRNTdvSlb93E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716697752548,"user_tz":-180,"elapsed":300,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}},"outputId":"7b40ba48-c217-491b-e041-f2ded5ee8ab3"},"id":"iRNTdvSlb93E","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['order_item_id',\n"," 'order_id',\n"," 'product_id',\n"," 'order_item_quantity',\n"," 'product_discount',\n"," 'product_subdiscount',\n"," 'product_price',\n"," 'product_subprice']"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"iQHdb89c2Y3p"},"id":"iQHdb89c2Y3p"},{"cell_type":"markdown","source":["Load Fact_order"],"metadata":{"id":"iyc_qtpg5EKa"},"id":"iyc_qtpg5EKa"},{"cell_type":"code","source":["def load_data(df, table_name):\n","    \"\"\"Load the transformed data into the target table in the data warehouse.\"\"\"\n","    # Menjelaskan bahwa fungsi ini bertujuan untuk memuat data yang telah ditransformasi ke dalam tabel target di dalam data warehouse.\n","\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # sa.create_engine(warehouse_conn_string): Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Cek kunci unique\n","        unique_key = get_unique_key(table_name)  # Misalnya user_id untuk tabel dim_user\n","        existing_data = pd.read_sql(f\"SELECT {unique_key} FROM {table_name}\", conn)\n","        # Mengecek Kunci Unik: Menggunakan fungsi get_unique_key untuk mendapatkan kunci unik dari tabel target yang dijadikan acuan. Ini bisa misalnya user_id untuk tabel dimensi dim_user.\n","        # Membaca Data yang Sudah Ada: Membaca data yang sudah ada di dalam tabel target dengan memilih hanya kolom kunci unik. Data ini akan digunakan untuk deduplikasi data yang baru dimuat.\n","\n","        # Deduplikasi data\n","        df = deduplicate_data(df, existing_data, unique_key)\n","        # Deduplikasi Data: Memanggil fungsi deduplicate_data untuk melakukan deduplikasi data baru (df) dengan data yang sudah ada di dalam tabel target.\n","        # Ini bertujuan untuk memastikan bahwa tidak ada data yang sama di dalam tabel target.\n","\n","        # Masukkan data baru\n","        df.to_sql(table_name, conn, index=False, if_exists='append', method='multi')\n","        # Memasukkan Data Baru: Memasukkan data baru yang telah ditransformasi (df) ke dalam tabel target di dalam database data warehouse. Pengaturan yang digunakan adalah:\n","        # a. table_name: Nama tabel target di dalam data warehouse.\n","        # b. conn: Koneksi ke database.\n","        # c. index=False: Tidak menyertakan indeks DataFrame dalam tabel yang dimuat.\n","        # d. if_exists='append': Data baru akan ditambahkan ke dalam tabel yang sudah ada.\n","        # e. method='multi': Metode yang digunakan untuk memasukkan data, dalam hal ini menggunakan metode multi-baris untuk meningkatkan efisiensi memuat data.\n","\n","\n","        print(f'Load Data {table_name} Success')"],"metadata":{"id":"kUIJtmU12m00"},"id":"kUIJtmU12m00","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fungsi load_data bertanggung jawab untuk memuat data yang telah ditransformasi ke dalam tabel target di dalam data warehouse. Langkah-langkah yang dilakukan meliputi memeriksa kunci unik, deduplikasi data, memasukkan data baru ke dalam tabel target, dan mencetak pesan sukses.\n","\n","Jadi, fungsi ini merupakan bagian penting dari proses ETL (Extract, Transform, Load) yang bertugas untuk memastikan data yang telah diolah dimuat dengan benar ke dalam data warehouse."],"metadata":{"id":"cmN9EAEGp8Uw"},"id":"cmN9EAEGp8Uw"},{"cell_type":"markdown","source":["Function untuk cheklist duplikat"],"metadata":{"id":"VEd50I3t51ZZ"},"id":"VEd50I3t51ZZ"},{"cell_type":"markdown","source":["Sudah melengkapi bagian dim_product_category, dim_products, fact_order_items!"],"metadata":{"id":"38IYCDkA6Nxm"},"id":"38IYCDkA6Nxm"},{"cell_type":"code","source":["def deduplicate_data(new_data, existing_data, unique_key):\n","    \"\"\"Remove duplicates from new data based on existing data.\"\"\"\n","    # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk menghapus duplikasi data dari data baru berdasarkan data yang sudah ada.\n","\n","    existing_keys = existing_data[unique_key].tolist()\n","    unique_rows = new_data[~new_data[unique_key].isin(existing_keys)]\n","    return unique_rows\n","    # Menghapus Duplikasi: Fungsi ini bekerja dengan mengambil kolom kunci unik dari data yang sudah ada dan mengonversinya menjadi daftar.\n","    # Selanjutnya, ia membandingkan kolom kunci unik dari data baru dengan daftar kunci yang ada, dan mengembalikan baris-baris yang tidak memiliki kunci yang sama dengan data yang sudah ada.\n","    # Dengan cara ini, duplikasi dihilangkan dan hanya baris unik yang dipertahankan.\n","\n","\n","\n","def get_unique_key(table_name):\n","    \"\"\"Retrieve the unique key of the table.\"\"\"\n","    # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk mengambil kunci unik dari tabel yang ditentukan.\n","\n","    if table_name == 'dim_user':\n","        return 'user_id'\n","    elif table_name == 'dim_payment':\n","        return 'payment_id'\n","    elif table_name == 'dim_shipper':\n","        return 'shipper_id'\n","    elif table_name == 'dim_rating':\n","        return 'rating_id'\n","    elif table_name == 'dim_voucher':\n","        return 'voucher_id'\n","    elif table_name == 'fact_orders':\n","        return 'order_id'\n","    elif table_name == 'dim_product_category':\n","        return 'product_category_id'\n","    elif table_name == 'dim_products':\n","        return 'product_id'\n","    elif table_name == 'fact_order_items':\n","        return 'order_item_id'\n","        # Tambahkan kondisi lain jika ada tabel lain\n","    else:\n","        raise ValueError(\"Table name not recognized.\")\n","\n","    # Kunci Unik Tabel: Fungsi ini mengembalikan kunci unik yang sesuai untuk tabel yang diberikan.\n","    # Ini dilakukan dengan memeriksa nama tabel yang diberikan dan mengembalikan kunci unik yang sesuai berdasarkan tabel tersebut. Jika nama tabel tidak dikenali, fungsi akan memunculkan ValueError.\n","\n"],"metadata":{"id":"XqXV7_4L51zt"},"id":"XqXV7_4L51zt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Potongan kode yang diberikan mengandung dua fungsi yang saling terkait untuk menangani penghapusan duplikasi data dan untuk mendapatkan kunci unik dari tabel.\n","\n","Kedua fungsi ini bekerja bersama-sama dalam proses ETL (Extract, Transform, Load) untuk menghapus duplikasi data dan menentukan kunci unik dari tabel yang berbeda. Fungsi deduplicate_data bertanggung jawab untuk menghapus duplikasi data berdasarkan kunci unik yang ada, sementara fungsi get_unique_key bertanggung jawab untuk memberikan kunci unik yang sesuai dengan tabel yang diberikan."],"metadata":{"id":"M_WawCvqp-iQ"},"id":"M_WawCvqp-iQ"},{"cell_type":"markdown","source":["### FULL FUNCTION"],"metadata":{"id":"NcdJSunL55qB"},"id":"NcdJSunL55qB"},{"cell_type":"markdown","source":["LENGKAPIN YANG get_unique_key(table_name) !"],"metadata":{"id":"iBjkkVAA6JJV"},"id":"iBjkkVAA6JJV"},{"cell_type":"code","execution_count":null,"id":"7ff56c09-846c-49c5-84c3-1491ffd98082","metadata":{"tags":[],"id":"7ff56c09-846c-49c5-84c3-1491ffd98082"},"outputs":[],"source":["# Buat tabel DWH fact dan dim nya\n","#def create_tables():\n","#    \"\"\"Create tables in the data warehouse if they do not exist.\"\"\"\n","#    engine = sa.create_engine(warehouse_conn_string) # create table baru\n","#    with engine.connect() as conn: # tes koneksi\n","#        for ddl in ddl_statements.values():\n","#            conn.execute(ddl) # di running tablenya\n","#\n","#def extract_data(table_name):\n","#    \"\"\"Extract data from a table in the OLTP database.\"\"\"\n","#    engine = sa.create_engine(oltp_conn_string)\n","#    query = f\"SELECT * FROM {oltp_tables[table_name]}\"\n","#    df = pd.read_sql(query, engine)\n","#    print(f'Extract Data {oltp_tables[table_name]} Success')\n","#    return df\n","\n","#def transform_data(df, target_table):\n","#    \"\"\"Transform the extracted data to match the schema of the target dimension table.\"\"\"\n","#    columns = dimension_columns.get(target_table)\n","#    if columns:\n","#        df = df[columns]\n","#    print(f'Transform Data {target_table} Success')\n","#    return df\n","\n","#def transform_fact_orders():\n","#    \"\"\"Transform data for the fact_orders table.\"\"\"\n","#    dataframes = {table: extract_data(table) for table in oltp_tables.keys()}\n","\n","#    df_orders = dataframes['orders']\n","#    df_orders = df_orders.merge(dataframes['users'], on='user_id')\n","#    df_orders = df_orders.merge(dataframes['payments'], on='payment_id')\n","#    df_orders = df_orders.merge(dataframes['shippers'], on='shipper_id')\n","#    df_orders = df_orders.merge(dataframes['ratings'], on='rating_id')\n","#    df_orders = df_orders.merge(dataframes['vouchers'], how='left', on='voucher_id')\n","#    df_orders.rename(columns={'user_id_x': 'user_id'}, inplace=True)\n","\n","#    fact_orders_columns = dimension_columns.get('fact_orders')\n","#    return df_orders[fact_orders_columns]\n","\n","\n","#def load_data(df, table_name):\n","#    \"\"\"Load the transformed data into the target table in the data warehouse.\"\"\"\n","#    engine = sa.create_engine(warehouse_conn_string)\n","#    with engine.connect() as conn:\n","#        # Cek kunci unique\n","#        unique_key = get_unique_key(table_name)  # Misalnya user_id untuk tabel dim_user\n","#        existing_data = pd.read_sql(f\"SELECT {unique_key} FROM {table_name}\", conn)\n","\n","        # Deduplikasi data\n","#        df = deduplicate_data(df, existing_data, unique_key)\n","\n","        # Masukkan data baru\n","#        df.to_sql(table_name, conn, index=False, if_exists='append', method='multi')\n","#        print(f'Load Data {table_name} Success')\n","\n","#def deduplicate_data(new_data, existing_data, unique_key):\n","#    \"\"\"Remove duplicates from new data based on existing data.\"\"\"\n","#    existing_keys = existing_data[unique_key].tolist()\n","#    unique_rows = new_data[~new_data[unique_key].isin(existing_keys)]\n","#    return unique_rows\n","\n","#def get_unique_key(table_name):\n","#    \"\"\"Retrieve the unique key of the table.\"\"\"\n","#    if table_name == 'dim_user':\n","#        return 'user_id'\n","#    elif table_name == 'dim_payment':\n","#        return 'payment_id'\n","#    elif table_name == 'dim_shipper':\n","#        return 'shipper_id'\n","#    elif table_name == 'dim_rating':\n","#        return 'rating_id'\n","#    elif table_name == 'dim_voucher':\n","#        return 'voucher_id'\n","#    elif table_name == 'fact_orders':\n","#        return 'order_id'\n","    # Tambahkan kondisi lain jika ada tabel lain\n","#    else:\n","#        raise ValueError(\"Table name not recognized.\")"]},{"cell_type":"markdown","id":"a8a0dab5-2f9d-472d-8350-ad46d2e7e60a","metadata":{"tags":[],"id":"a8a0dab5-2f9d-472d-8350-ad46d2e7e60a"},"source":["### **Function Data Mart**"]},{"cell_type":"markdown","source":["Potongan kode ini adalah fungsi Python yang bertanggung jawab untuk membuat tabel dm_sales di dalam data warehouse dan memasukkan data ke dalamnya\n","\n","Fungsi create_and_insert_dm_sales bertanggung jawab untuk membuat tabel dm_sales di dalam data warehouse dan memasukkan data ke dalamnya. Langkah-langkah yang dilakukan meliputi membuat koneksi ke data warehouse, membuat tabel dm_sales, memasukkan data ke dalamnya, dan mencetak pesan sukses setelah proses selesai. Jadi, fungsi ini merupakan bagian penting dari proses ETL (Extract, Transform, Load) yang bertugas untuk mempersiapkan data dalam bentuk yang sesuai untuk analisis di dalam data warehouse."],"metadata":{"id":"zeeXrpDtqGPJ"},"id":"zeeXrpDtqGPJ"},{"cell_type":"markdown","source":["#### 1.) dm_sales"],"metadata":{"id":"ku98V4PRPRMh"},"id":"ku98V4PRPRMh"},{"cell_type":"code","execution_count":null,"id":"0d93de0d-ea8e-422d-aa68-1c1132427704","metadata":{"tags":[],"id":"0d93de0d-ea8e-422d-aa68-1c1132427704"},"outputs":[],"source":["def create_and_insert_dm_sales():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_1['dim_sales'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_1['insert_dm_sales'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"]},{"cell_type":"markdown","source":["#### 2.) dm_product_sales_2"],"metadata":{"id":"BcMJZycDPeIL"},"id":"BcMJZycDPeIL"},{"cell_type":"code","source":["def create_and_insert_dm_product_sales_2():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_2['dm_product_sales_2'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_2['insert_dm_product_sales_2'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"o-pGVfV3Pd-6"},"id":"o-pGVfV3Pd-6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.) dm_payment_sales_3"],"metadata":{"id":"WWlzxVdIPyWg"},"id":"WWlzxVdIPyWg"},{"cell_type":"code","source":["def create_and_insert_dm_payment_sales_3():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_3['dm_payment_sales_3'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_3['insert_dm_payment_sales_3'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"CibZ-uu_PyKo"},"id":"CibZ-uu_PyKo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4.) dm_shipper_sales_4"],"metadata":{"id":"atnvcLdBQNrf"},"id":"atnvcLdBQNrf"},{"cell_type":"code","source":["def create_and_insert_dm_shipper_sales_4 ():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_4['dm_shipper_sales_4'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_4['insert_dm_shipper_sales_4'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"p3LRrW_gQNSE"},"id":"p3LRrW_gQNSE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5.) dm_user_sales_5"],"metadata":{"id":"v7PwRkKYQaKA"},"id":"v7PwRkKYQaKA"},{"cell_type":"code","source":["def create_and_insert_dm_user_sales_5():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_5['dm_user_sales_5'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_5['insert_dm_user_sales_5'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"SiFXbZGJQZ_E"},"id":"SiFXbZGJQZ_E","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6.) dm_discount_voucher_trend_6"],"metadata":{"id":"_0D3JQNZQ5Px"},"id":"_0D3JQNZQ5Px"},{"cell_type":"code","source":["def create_and_insert_dm_discount_voucher_trend_6():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_6['dm_discount_voucher_trend_6'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_6['insert_dm_discount_voucher_trend_6'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"2QLMHF4JQ5Dg"},"id":"2QLMHF4JQ5Dg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7.) dm_sales_performance_by_region_7"],"metadata":{"id":"okA_EPXARMX7"},"id":"okA_EPXARMX7"},{"cell_type":"code","source":["def create_and_insert_dm_sales_performance_by_region_7():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_7['dm_sales_performance_by_region_7'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_7['insert_dm_sales_performance_by_region_7'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"gchCvi8ZRMM_"},"id":"gchCvi8ZRMM_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8.) dm_profit_margin_per_category_8"],"metadata":{"id":"OETkshP-RZWs"},"id":"OETkshP-RZWs"},{"cell_type":"code","source":["def create_and_insert_dm_profit_margin_per_category_8():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_8['dm_profit_margin_per_category_8'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_8['insert_dm_profit_margin_per_category_8'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"iqDnjWvJRZMR"},"id":"iqDnjWvJRZMR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 9.) dm_average_order_value_per_user_9"],"metadata":{"id":"pSCthSotRosA"},"id":"pSCthSotRosA"},{"cell_type":"code","source":["def create_and_insert_dm_average_order_value_per_user_9():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_9['dm_average_order_value_per_user_9'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_9['insert_dm_average_order_value_per_user_9'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"gMg5qa6xRoj3"},"id":"gMg5qa6xRoj3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10.) dm_voucher_conversion_rate_10"],"metadata":{"id":"sABwUzgbR4A7"},"id":"sABwUzgbR4A7"},{"cell_type":"code","source":["def create_and_insert_dm_voucher_conversion_rate_10():\n","  # Docstring: Menjelaskan bahwa fungsi ini bertujuan untuk membuat tabel dm_sales dan memasukkan data ke dalamnya di dalam data warehouse.\n","\n","    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    # Membuat Engine: Menggunakan SQLAlchemy (sa kemungkinan adalah alias dari sqlalchemy) untuk membuat engine yang akan digunakan untuk koneksi ke database data warehouse.\n","    # warehouse_conn_string adalah string koneksi yang berisi informasi tentang bagaimana menghubungkan ke database data warehouse.\n","\n","\n","    with engine.connect() as conn:\n","        # Create dm_sales table\n","        conn.execute(ddl_marts_10['dm_voucher_conversion_rate_10'])\n","        # Membuat Tabel: Menggunakan koneksi (conn) untuk mengeksekusi pernyataan DDL (Data Definition Language) yang disimpan dalam variabel ddl_marts['dim_sales'].\n","        # Ini bertujuan untuk membuat tabel dm_sales di dalam database data warehouse.\n","\n","        # Insert data into dm_sales table\n","        conn.execute(ddl_marts_10['insert_dm_voucher_conversion_rate_10'])\n","        # Memasukkan Data: Setelah tabel dm_sales dibuat, data dimasukkan ke dalamnya dengan mengeksekusi pernyataan DML (Data Manipulation Language) yang disimpan dalam variabel ddl_marts['insert_dm_sales'].\n","\n","\n","    print(f'Data Mart Has Create Success')"],"metadata":{"id":"0i2Kc0b-R34U"},"id":"0i2Kc0b-R34U","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f09cd1cd-e9ac-4a99-91f2-f3b9dcc28312","metadata":{"id":"f09cd1cd-e9ac-4a99-91f2-f3b9dcc28312"},"source":["### **Function Run ETL**"]},{"cell_type":"code","execution_count":null,"id":"10330d98-0373-42aa-8307-caa46e6bf0ec","metadata":{"tags":[],"id":"10330d98-0373-42aa-8307-caa46e6bf0ec"},"outputs":[],"source":["def etl_process():\n","    \"\"\"Run the entire ETL process.\"\"\"\n","    # Menjelaskan bahwa fungsi ini bertujuan untuk menjalankan keseluruhan proses ETL.\n","\n","    # Create tables\n","    create_tables()\n","    # Membuat Tabel: Memanggil fungsi create_tables yang bertugas untuk membuat tabel-tabel yang diperlukan di dalam data warehouse.\n","\n","    # Process dimension tables\n","    for dim_table, target_table in warehouse_tables.items():\n","        if dim_table != 'fact_orders':\n","            source_table = dim_table\n","            df = extract_data(source_table)\n","            transformed_df = transform_data(df, dim_table)\n","            load_data(transformed_df, target_table)\n","        else:\n","            # Process fact table\n","            df_fact_orders = transform_fact_orders()\n","            load_data(df_fact_orders, target_table)\n","\n","    # Memproses Tabel Dimensi: Melalui loop, fungsi ini melakukan iterasi melalui setiap tabel dimensi yang didefinisikan dalam warehouse_tables.\n","    # Jika tabel yang sedang diproses bukanlah tabel fakta (fact_orders), maka data diekstrak, ditransformasi, dan dimuat ke dalam data warehouse.\n","    # Jika tabel yang sedang diproses adalah tabel fakta, maka fungsi transform_fact_orders dipanggil untuk mentransformasi data faktual, dan hasilnya dimuat ke dalam data warehouse."]},{"cell_type":"markdown","source":["Potongan kode tersebut adalah sebuah fungsi Python yang bertanggung jawab untuk menjalankan keseluruhan proses ETL (Extract, Transform, Load). Fungsi ini berisi serangkaian langkah-langkah yang diperlukan untuk mempersiapkan dan memuat data dari sumbernya ke dalam data warehouse\n","\n","Fungsi etl_process adalah inti dari proses ETL yang bertanggung jawab untuk menjalankan semua langkah yang diperlukan dalam mempersiapkan dan memuat data ke dalam data warehouse. Ini mencakup pembuatan tabel, pemrosesan tabel dimensi dan fakta, serta pembuatan dan pengisian data ke dalam tabel Mart. Dengan menjalankan fungsi ini, keseluruhan proses ETL dapat dieksekusi dengan efisien dan konsisten."],"metadata":{"id":"Rg4tXWjAqLQC"},"id":"Rg4tXWjAqLQC"},{"cell_type":"markdown","source":["#### 1.) Run dm_sales"],"metadata":{"id":"FsLevjYASPYl"},"id":"FsLevjYASPYl"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_sales()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"j-_d7-c5SOBG"},"id":"j-_d7-c5SOBG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2.) Run dm_product_sales_2"],"metadata":{"id":"foZspMI5Shfc"},"id":"foZspMI5Shfc"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_product_sales_2()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"I3jOhcexShVp"},"id":"I3jOhcexShVp","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.) Run dm_payment_sales_3"],"metadata":{"id":"phNPJIqiTIrX"},"id":"phNPJIqiTIrX"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_payment_sales_3()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"3CCYuAV3SuyL"},"execution_count":null,"outputs":[],"id":"3CCYuAV3SuyL"},{"cell_type":"markdown","source":["#### 4.) Run dm_shipper_sales_4"],"metadata":{"id":"QCKXVwlnTMhh"},"id":"QCKXVwlnTMhh"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_shipper_sales_4()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"c81mjmeJSwGP"},"execution_count":null,"outputs":[],"id":"c81mjmeJSwGP"},{"cell_type":"markdown","source":["#### 5.) Run dm_user_sales_5"],"metadata":{"id":"hIEJ4LB6TQIW"},"id":"hIEJ4LB6TQIW"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_user_sales_5()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"Mc-9N3xvSwN6"},"execution_count":null,"outputs":[],"id":"Mc-9N3xvSwN6"},{"cell_type":"markdown","source":["#### 6.) Run dm_discount_voucher_trend_6"],"metadata":{"id":"3Pg3Xgd4TUYx"},"id":"3Pg3Xgd4TUYx"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_discount_voucher_trend_6()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"adCzwUeuSwTi"},"execution_count":null,"outputs":[],"id":"adCzwUeuSwTi"},{"cell_type":"markdown","source":["#### 7.) Run dm_sales_performance_by_region_7"],"metadata":{"id":"fEf_1kRWTYmd"},"id":"fEf_1kRWTYmd"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_sales_performance_by_region_7()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"O5ZrlGg2SwXx"},"execution_count":null,"outputs":[],"id":"O5ZrlGg2SwXx"},{"cell_type":"markdown","source":["#### 8.) Run dm_profit_margin_per_category_8"],"metadata":{"id":"GURu9WbpTbta"},"id":"GURu9WbpTbta"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_profit_margin_per_category_8()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"gXF1NwZiSwbf"},"execution_count":null,"outputs":[],"id":"gXF1NwZiSwbf"},{"cell_type":"markdown","source":["#### 9.) Run dm_average_order_value_per_user_9"],"metadata":{"id":"zyBn4nc9TfE0"},"id":"zyBn4nc9TfE0"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_average_order_value_per_user_9()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"43Szl61SSwfJ"},"execution_count":null,"outputs":[],"id":"43Szl61SSwfJ"},{"cell_type":"markdown","source":["#### 10.) Run dm_voucher_conversion_rate_10"],"metadata":{"id":"U01JrS_VTiuR"},"id":"U01JrS_VTiuR"},{"cell_type":"code","source":["# proses mart table\n","create_and_insert_dm_voucher_conversion_rate_10()\n","# Memproses Tabel Mart: Setelah selesai memproses semua tabel dimensi dan fakta,\n","# fungsi create_and_insert_dm_sales dipanggil untuk membuat dan memasukkan data ke dalam tabel Mart (dm_sales), yang kemungkinan adalah agregasi atau transformasi data untuk tujuan analisis."],"metadata":{"id":"YBZLydVvTEVL"},"execution_count":null,"outputs":[],"id":"YBZLydVvTEVL"},{"cell_type":"markdown","id":"615f468a-fb5f-476f-a7d0-5cca7d859a96","metadata":{"id":"615f468a-fb5f-476f-a7d0-5cca7d859a96"},"source":["# **Run ETL**"]},{"cell_type":"code","execution_count":null,"id":"2738ed9d-6512-4f11-b7be-400976529462","metadata":{"tags":[],"id":"2738ed9d-6512-4f11-b7be-400976529462","outputId":"1150253f-31e3-4351-b1f4-d914bb775686","colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"status":"error","timestamp":1716699493825,"user_tz":-180,"elapsed":4104,"user":{"displayName":"Agung Krishna","userId":"17111598879644679482"}}},"outputs":[{"output_type":"error","ename":"ObjectNotExecutableError","evalue":"Not an executable object: '\\n       CREATE TABLE IF NOT EXISTS dim_users (\\n            user_id INT NOT NULL PRIMARY KEY,\\n            user_first_name VARCHAR(255) NOT NULL,\\n            user_last_name VARCHAR(255) NOT NULL,\\n            user_gender VARCHAR(50) NOT NULL,\\n            user_address VARCHAR(255),\\n            user_birthdate DATE NOT NULL,\\n            user_join DATE NOT NULL\\n        );\\n    '","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-2becdffa6c8e>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#script running all ETL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0metl_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-cf26c34d3878>\u001b[0m in \u001b[0;36metl_process\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcreate_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Membuat Tabel: Memanggil fungsi create_tables yang bertugas untuk membuat tabel-tabel yang diperlukan di dalam data warehouse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-2becdffa6c8e>\u001b[0m in \u001b[0;36mcreate_tables\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# tes koneksi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mddl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddl_statements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# di running tablenya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m             return meth(\n","\u001b[0;31mObjectNotExecutableError\u001b[0m: Not an executable object: '\\n       CREATE TABLE IF NOT EXISTS dim_users (\\n            user_id INT NOT NULL PRIMARY KEY,\\n            user_first_name VARCHAR(255) NOT NULL,\\n            user_last_name VARCHAR(255) NOT NULL,\\n            user_gender VARCHAR(50) NOT NULL,\\n            user_address VARCHAR(255),\\n            user_birthdate DATE NOT NULL,\\n            user_join DATE NOT NULL\\n        );\\n    '"]}],"source":["# Buat tabel DWH fact dan dim nya\n","def create_tables():\n","    \"\"\"Create tables in the data warehouse if they do not exist.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string) # create table baru\n","    with engine.connect() as conn: # tes koneksi\n","        for ddl in ddl_statements.values():\n","            conn.execute(ddl) # di running tablenya\n","\n","def extract_data(table_name):\n","    \"\"\"Extract data from a table in the OLTP database.\"\"\"\n","    engine = sa.create_engine(oltp_conn_string)\n","    query = f\"SELECT * FROM {oltp_tables[table_name]}\"\n","    df = pd.read_sql(query, engine)\n","    print(f'Extract Data {oltp_tables[table_name]} Success')\n","    return df\n","\n","def transform_data(df, target_table):\n","    \"\"\"Transform the extracted data to match the schema of the target dimension table.\"\"\"\n","    columns = dimension_columns.get(target_table)\n","    if columns:\n","        df = df[columns]\n","    print(f'Transform Data {target_table} Success')\n","    return df\n","\n","def transform_fact_orders():\n","    \"\"\"Transform data for the fact_orders table.\"\"\"\n","    dataframes = {table: extract_data(table) for table in oltp_tables.keys()}\n","\n","    df_orders = dataframes['orders']\n","    df_orders = df_orders.merge(dataframes['users'], on='user_id')\n","    df_orders = df_orders.merge(dataframes['payments'], on='payment_id')\n","    df_orders = df_orders.merge(dataframes['shippers'], on='shipper_id')\n","    df_orders = df_orders.merge(dataframes['ratings'], on='rating_id')\n","    df_orders = df_orders.merge(dataframes['vouchers'], how='left', on='voucher_id')\n","    df_orders.rename(columns={'user_id_x': 'user_id'}, inplace=True)\n","\n","    fact_orders_columns = dimension_columns.get('fact_orders')\n","    return df_orders[fact_orders_columns]\n","\n","\n","def load_data(df, table_name):\n","    \"\"\"Load the transformed data into the target table in the data warehouse.\"\"\"\n","    engine = sa.create_engine(warehouse_conn_string)\n","    with engine.connect() as conn:\n","        # Cek kunci unique\n","        unique_key = get_unique_key(table_name)  # Misalnya user_id untuk tabel dim_user\n","        existing_data = pd.read_sql(f\"SELECT {unique_key} FROM {table_name}\", conn)\n","\n","        # Deduplikasi data\n","        df = deduplicate_data(df, existing_data, unique_key)\n","\n","        # Masukkan data baru\n","        df.to_sql(table_name, conn, index=False, if_exists='append', method='multi')\n","        print(f'Load Data {table_name} Success')\n","\n","def deduplicate_data(new_data, existing_data, unique_key):\n","    \"\"\"Remove duplicates from new data based on existing data.\"\"\"\n","    existing_keys = existing_data[unique_key].tolist()\n","    unique_rows = new_data[~new_data[unique_key].isin(existing_keys)]\n","    return unique_rows\n","\n","def get_unique_key(table_name):\n","    \"\"\"Retrieve the unique key of the table.\"\"\"\n","    if table_name == 'dim_user':\n","        return 'user_id'\n","    elif table_name == 'dim_payment':\n","        return 'payment_id'\n","    elif table_name == 'dim_shipper':\n","        return 'shipper_id'\n","    elif table_name == 'dim_rating':\n","        return 'rating_id'\n","    elif table_name == 'dim_voucher':\n","        return 'voucher_id'\n","    elif table_name == 'fact_orders':\n","        return 'order_id'\n","    # Tambahkan kondisi lain jika ada tabel lain\n","    else:\n","        raise ValueError(\"Table name not recognized.\")\n","\n","#script running all ETL\n","etl_process()"]},{"cell_type":"markdown","id":"ad1e1fec-86cb-4426-bc7b-28b5ab27efec","metadata":{"id":"ad1e1fec-86cb-4426-bc7b-28b5ab27efec"},"source":["### **Run Testing**"]},{"cell_type":"code","source":["source_table"],"metadata":{"id":"GeM9jT-w8c2U"},"id":"GeM9jT-w8c2U","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2ba73e31-b2fe-478c-bad1-b756ad924fb9","metadata":{"tags":[],"id":"2ba73e31-b2fe-478c-bad1-b756ad924fb9","outputId":"49731771-744f-4dc1-d23e-01829e64a59a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extract Data tb_users Success\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>user_first_name</th>\n","      <th>user_last_name</th>\n","      <th>user_gender</th>\n","      <th>user_address</th>\n","      <th>user_birthday</th>\n","      <th>user_join</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100101</td>\n","      <td>Budi</td>\n","      <td>Gunawan</td>\n","      <td>Male</td>\n","      <td>Jl. Pondok Indah No.1, Kecamatan Pondok Labu, ...</td>\n","      <td>1998-09-12</td>\n","      <td>2022-01-13</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100102</td>\n","      <td>Eva</td>\n","      <td>Susanti</td>\n","      <td>Female</td>\n","      <td>Jl. Timur Raya No. 13, Kramat Jaya, Jakarta Ti...</td>\n","      <td>1997-02-16</td>\n","      <td>2022-01-29</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100103</td>\n","      <td>Dana</td>\n","      <td>Pradana</td>\n","      <td>Male</td>\n","      <td>Jl. Pahlawan, Surabaya, Jawa Timur</td>\n","      <td>1999-07-19</td>\n","      <td>2022-02-11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100104</td>\n","      <td>Rahmat</td>\n","      <td>Hidayat</td>\n","      <td>Male</td>\n","      <td>Jl. Amil Abas, Jakarta Timur, DKI Jakarta</td>\n","      <td>2000-02-14</td>\n","      <td>2022-03-22</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100105</td>\n","      <td>Dodo</td>\n","      <td>Andriano</td>\n","      <td>Male</td>\n","      <td>Jl. Pakuan Selatan No. 177, Magelang, Jawa Tengah</td>\n","      <td>2000-09-06</td>\n","      <td>2022-04-03</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>100106</td>\n","      <td>Caca</td>\n","      <td>Kumala</td>\n","      <td>Female</td>\n","      <td>Jl. Bunga Raya, Kota Tanggerang, Banten</td>\n","      <td>1998-11-05</td>\n","      <td>2022-05-20</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>100107</td>\n","      <td>Andi</td>\n","      <td>Kurniawan</td>\n","      <td>Male</td>\n","      <td>Jl. Mawar Indah No. 25, Jakarta Barat, DKI Jak...</td>\n","      <td>2001-03-14</td>\n","      <td>2022-05-24</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>100108</td>\n","      <td>Fanny</td>\n","      <td>Utami</td>\n","      <td>Female</td>\n","      <td>Jl. Kilometer Panjang No. 210, Jakarta Utara, ...</td>\n","      <td>2002-01-27</td>\n","      <td>2022-06-02</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>100109</td>\n","      <td>Gagah</td>\n","      <td>Prakasa</td>\n","      <td>Male</td>\n","      <td>Jl. Timur Asri No. 10, Denpasar, Bali</td>\n","      <td>2001-08-05</td>\n","      <td>2022-07-14</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>100110</td>\n","      <td>Anita</td>\n","      <td>Friska</td>\n","      <td>Female</td>\n","      <td>Jl. Tembung Raya, Kota Medan Timur, Sumatera U...</td>\n","      <td>2000-11-04</td>\n","      <td>2022-07-21</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   user_id user_first_name user_last_name user_gender  \\\n","0   100101            Budi        Gunawan        Male   \n","1   100102             Eva        Susanti      Female   \n","2   100103            Dana        Pradana        Male   \n","3   100104          Rahmat        Hidayat        Male   \n","4   100105            Dodo       Andriano        Male   \n","5   100106            Caca         Kumala      Female   \n","6   100107            Andi      Kurniawan        Male   \n","7   100108           Fanny          Utami      Female   \n","8   100109           Gagah        Prakasa        Male   \n","9   100110           Anita         Friska      Female   \n","\n","                                        user_address user_birthday   user_join  \n","0  Jl. Pondok Indah No.1, Kecamatan Pondok Labu, ...    1998-09-12  2022-01-13  \n","1  Jl. Timur Raya No. 13, Kramat Jaya, Jakarta Ti...    1997-02-16  2022-01-29  \n","2                 Jl. Pahlawan, Surabaya, Jawa Timur    1999-07-19  2022-02-11  \n","3          Jl. Amil Abas, Jakarta Timur, DKI Jakarta    2000-02-14  2022-03-22  \n","4  Jl. Pakuan Selatan No. 177, Magelang, Jawa Tengah    2000-09-06  2022-04-03  \n","5            Jl. Bunga Raya, Kota Tanggerang, Banten    1998-11-05  2022-05-20  \n","6  Jl. Mawar Indah No. 25, Jakarta Barat, DKI Jak...    2001-03-14  2022-05-24  \n","7  Jl. Kilometer Panjang No. 210, Jakarta Utara, ...    2002-01-27  2022-06-02  \n","8              Jl. Timur Asri No. 10, Denpasar, Bali    2001-08-05  2022-07-14  \n","9  Jl. Tembung Raya, Kota Medan Timur, Sumatera U...    2000-11-04  2022-07-21  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["create_tables()\n","\n","source_table = 'users'\n","df = extract_data(source_table)\n","df"]},{"cell_type":"code","execution_count":null,"id":"6af6dfee-7af4-412b-9dc4-537ec3689526","metadata":{"tags":[],"id":"6af6dfee-7af4-412b-9dc4-537ec3689526","outputId":"b1825ab3-a985-4b19-e5bb-4b04311525ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Transform Data dim_user Success\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>user_first_name</th>\n","      <th>user_last_name</th>\n","      <th>user_gender</th>\n","      <th>user_address</th>\n","      <th>user_birthday</th>\n","      <th>user_join</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100101</td>\n","      <td>Budi</td>\n","      <td>Gunawan</td>\n","      <td>Male</td>\n","      <td>Jl. Pondok Indah No.1, Kecamatan Pondok Labu, ...</td>\n","      <td>1998-09-12</td>\n","      <td>2022-01-13</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100102</td>\n","      <td>Eva</td>\n","      <td>Susanti</td>\n","      <td>Female</td>\n","      <td>Jl. Timur Raya No. 13, Kramat Jaya, Jakarta Ti...</td>\n","      <td>1997-02-16</td>\n","      <td>2022-01-29</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100103</td>\n","      <td>Dana</td>\n","      <td>Pradana</td>\n","      <td>Male</td>\n","      <td>Jl. Pahlawan, Surabaya, Jawa Timur</td>\n","      <td>1999-07-19</td>\n","      <td>2022-02-11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100104</td>\n","      <td>Rahmat</td>\n","      <td>Hidayat</td>\n","      <td>Male</td>\n","      <td>Jl. Amil Abas, Jakarta Timur, DKI Jakarta</td>\n","      <td>2000-02-14</td>\n","      <td>2022-03-22</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100105</td>\n","      <td>Dodo</td>\n","      <td>Andriano</td>\n","      <td>Male</td>\n","      <td>Jl. Pakuan Selatan No. 177, Magelang, Jawa Tengah</td>\n","      <td>2000-09-06</td>\n","      <td>2022-04-03</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>100106</td>\n","      <td>Caca</td>\n","      <td>Kumala</td>\n","      <td>Female</td>\n","      <td>Jl. Bunga Raya, Kota Tanggerang, Banten</td>\n","      <td>1998-11-05</td>\n","      <td>2022-05-20</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>100107</td>\n","      <td>Andi</td>\n","      <td>Kurniawan</td>\n","      <td>Male</td>\n","      <td>Jl. Mawar Indah No. 25, Jakarta Barat, DKI Jak...</td>\n","      <td>2001-03-14</td>\n","      <td>2022-05-24</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>100108</td>\n","      <td>Fanny</td>\n","      <td>Utami</td>\n","      <td>Female</td>\n","      <td>Jl. Kilometer Panjang No. 210, Jakarta Utara, ...</td>\n","      <td>2002-01-27</td>\n","      <td>2022-06-02</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>100109</td>\n","      <td>Gagah</td>\n","      <td>Prakasa</td>\n","      <td>Male</td>\n","      <td>Jl. Timur Asri No. 10, Denpasar, Bali</td>\n","      <td>2001-08-05</td>\n","      <td>2022-07-14</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>100110</td>\n","      <td>Anita</td>\n","      <td>Friska</td>\n","      <td>Female</td>\n","      <td>Jl. Tembung Raya, Kota Medan Timur, Sumatera U...</td>\n","      <td>2000-11-04</td>\n","      <td>2022-07-21</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   user_id user_first_name user_last_name user_gender  \\\n","0   100101            Budi        Gunawan        Male   \n","1   100102             Eva        Susanti      Female   \n","2   100103            Dana        Pradana        Male   \n","3   100104          Rahmat        Hidayat        Male   \n","4   100105            Dodo       Andriano        Male   \n","5   100106            Caca         Kumala      Female   \n","6   100107            Andi      Kurniawan        Male   \n","7   100108           Fanny          Utami      Female   \n","8   100109           Gagah        Prakasa        Male   \n","9   100110           Anita         Friska      Female   \n","\n","                                        user_address user_birthday   user_join  \n","0  Jl. Pondok Indah No.1, Kecamatan Pondok Labu, ...    1998-09-12  2022-01-13  \n","1  Jl. Timur Raya No. 13, Kramat Jaya, Jakarta Ti...    1997-02-16  2022-01-29  \n","2                 Jl. Pahlawan, Surabaya, Jawa Timur    1999-07-19  2022-02-11  \n","3          Jl. Amil Abas, Jakarta Timur, DKI Jakarta    2000-02-14  2022-03-22  \n","4  Jl. Pakuan Selatan No. 177, Magelang, Jawa Tengah    2000-09-06  2022-04-03  \n","5            Jl. Bunga Raya, Kota Tanggerang, Banten    1998-11-05  2022-05-20  \n","6  Jl. Mawar Indah No. 25, Jakarta Barat, DKI Jak...    2001-03-14  2022-05-24  \n","7  Jl. Kilometer Panjang No. 210, Jakarta Utara, ...    2002-01-27  2022-06-02  \n","8              Jl. Timur Asri No. 10, Denpasar, Bali    2001-08-05  2022-07-14  \n","9  Jl. Tembung Raya, Kota Medan Timur, Sumatera U...    2000-11-04  2022-07-21  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# TESTING SETIAP TABEL\n","\n","transformed_df = transform_data(df, 'dim_user')\n","transformed_df"]},{"cell_type":"code","execution_count":null,"id":"d0c95c03-3427-4248-a2ef-1dee7f556f9d","metadata":{"tags":[],"id":"d0c95c03-3427-4248-a2ef-1dee7f556f9d","outputId":"6128f1a3-474c-4e99-9f19-7071c91a35a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Data dim_user Success\n"]}],"source":["load_data(transformed_df, 'dim_user')"]},{"cell_type":"markdown","source":["PROSES SUDAH SELESAI, BISA REFRESH DATANYA DI POSTGRE DWH"],"metadata":{"id":"1X-bny4w9Lg5"},"id":"1X-bny4w9Lg5"},{"cell_type":"markdown","source":["RUNNING ULANG SETIAP TABELNYA DAN DILENGKAPIN APA YANG BELUM LENGKAP"],"metadata":{"id":"miHX80dP9Z8A"},"id":"miHX80dP9Z8A"},{"cell_type":"markdown","id":"63afee92-a119-4e39-9afd-5aec4b238e1b","metadata":{"id":"63afee92-a119-4e39-9afd-5aec4b238e1b"},"source":["### **Script Upload Google Sheets**"]},{"cell_type":"code","execution_count":null,"id":"86ce8176-06b3-46b7-9328-679fd00545bb","metadata":{"tags":[],"id":"86ce8176-06b3-46b7-9328-679fd00545bb"},"outputs":[],"source":["import json\n","import gspread\n","from oauth2client.service_account import ServiceAccountCredentials\n","\n","with open('digitalskola_key.json','rb') as file:\n","    key = json.load(file)\n","\n","scope = ['https://www.googleapis.com/auth/drive','https://spreadsheets.google.com/feeds']\n","creds = ServiceAccountCredentials.from_json_keyfile_dict(key, scope)\n","client = gspread.authorize(creds)\n","\n","###tambahkan email googledigitalskola@digitalskola-368401.iam.gserviceaccount.com ke dalam google sheet anda#"]},{"cell_type":"code","execution_count":null,"id":"155dce82-f692-450f-81e7-720013c3a3d8","metadata":{"tags":[],"id":"155dce82-f692-450f-81e7-720013c3a3d8","outputId":"d38f7bcb-1837-4a88-e8fb-e1b1718e0a01"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>order_id</th>\n","      <th>order_date</th>\n","      <th>user_id</th>\n","      <th>user_name</th>\n","      <th>payment_type</th>\n","      <th>shipper_name</th>\n","      <th>order_price</th>\n","      <th>order_discount</th>\n","      <th>voucher_name</th>\n","      <th>order_total</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1110001</td>\n","      <td>2022-01-20</td>\n","      <td>100101</td>\n","      <td>Budi Gunawan</td>\n","      <td>Debit</td>\n","      <td>JNE Express</td>\n","      <td>250000</td>\n","      <td>15000</td>\n","      <td>New User</td>\n","      <td>230000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1110002</td>\n","      <td>2022-01-29</td>\n","      <td>100102</td>\n","      <td>Eva Susanti</td>\n","      <td>Debit</td>\n","      <td>JNE Express</td>\n","      <td>620000</td>\n","      <td>40000</td>\n","      <td>New User</td>\n","      <td>575000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1110003</td>\n","      <td>2022-02-13</td>\n","      <td>100103</td>\n","      <td>Dana Pradana</td>\n","      <td>Credit</td>\n","      <td>JNE Express</td>\n","      <td>6000000</td>\n","      <td>1000000</td>\n","      <td>New User</td>\n","      <td>4995000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1110005</td>\n","      <td>2022-04-28</td>\n","      <td>100105</td>\n","      <td>Dodo Andriano</td>\n","      <td>Debit</td>\n","      <td>Sicepat Express</td>\n","      <td>4000000</td>\n","      <td>1000000</td>\n","      <td>New User</td>\n","      <td>2995000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1110008</td>\n","      <td>2022-06-02</td>\n","      <td>100108</td>\n","      <td>Fanny Utami</td>\n","      <td>Credit</td>\n","      <td>Sicepat Express</td>\n","      <td>2000000</td>\n","      <td>0</td>\n","      <td>New User</td>\n","      <td>1995000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1110012</td>\n","      <td>2022-07-30</td>\n","      <td>100110</td>\n","      <td>Anita Friska</td>\n","      <td>Debit</td>\n","      <td>JNE Express</td>\n","      <td>490000</td>\n","      <td>35000</td>\n","      <td>Body Soap Promo</td>\n","      <td>445000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   order_id  order_date  user_id      user_name payment_type     shipper_name  \\\n","0   1110001  2022-01-20   100101   Budi Gunawan        Debit      JNE Express   \n","1   1110002  2022-01-29   100102    Eva Susanti        Debit      JNE Express   \n","2   1110003  2022-02-13   100103   Dana Pradana       Credit      JNE Express   \n","3   1110005  2022-04-28   100105  Dodo Andriano        Debit  Sicepat Express   \n","4   1110008  2022-06-02   100108    Fanny Utami       Credit  Sicepat Express   \n","5   1110012  2022-07-30   100110   Anita Friska        Debit      JNE Express   \n","\n","   order_price  order_discount     voucher_name  order_total  \n","0       250000           15000         New User       230000  \n","1       620000           40000         New User       575000  \n","2      6000000         1000000         New User      4995000  \n","3      4000000         1000000         New User      2995000  \n","4      2000000               0         New User      1995000  \n","5       490000           35000  Body Soap Promo       445000  "]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["def fetch_data_from_dwh(query):\n","     # Membuat koneksi ke database\n","    engine = sa.create_engine(warehouse_conn_string)\n","\n","    # Membuat hasil query menjadi Datafrmae\n","    df = pd.read_sql(query, engine)\n","\n","    return df\n","\n","df_mart = fetch_data_from_dwh(\"\"\"SELECT * FROM dm_sales;\"\"\")\n","df_mart"]},{"cell_type":"code","execution_count":null,"id":"71476a8b-dde7-4794-93da-04c25d3faa0c","metadata":{"tags":[],"id":"71476a8b-dde7-4794-93da-04c25d3faa0c","outputId":"84f89469-8d27-4916-b9d3-5a3fffa7b8eb"},"outputs":[{"data":{"text/plain":["{'spreadsheetId': '163IyMV2W_SR_vYg9IOPYcmQwOtVxSfFQzhPPb9RjBA0',\n"," 'updatedRange': 'Sheet3!A1:J7',\n"," 'updatedRows': 7,\n"," 'updatedColumns': 10,\n"," 'updatedCells': 70}"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# ganti dengan nama google sheets anda\n","sheet = client.open('Contoh Source Data')\n","\n","# ganti sesuai dengan nama sheet didalam google sheets anda\n","# siapkan nama kolom pada sheet di google sheet anda\n","\n","export = sheet.worksheet('Sheet3')\n","export.update([df_mart.columns.values.tolist()] + df_mart.astype(str).values.tolist())"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}